import random
import re
import unicodedata
from collections import Counter
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import streamlit as st
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler


st.set_page_config(
    page_title="AI vs Human Text Classifier",
    page_icon="ğŸ§ ",
    layout="wide",
)

# Initialize default text once so sample buttons can override it reliably.
if "input_text" not in st.session_state:
    st.session_state["input_text"] = (
        "I spent the afternoon finishing my report, then rewrote the introduction to make it clearer."
    )


# English + Chinese stopwords for rough style cues.
STOP_WORDS = {
    "a",
    "an",
    "and",
    "are",
    "as",
    "at",
    "be",
    "but",
    "by",
    "for",
    "if",
    "in",
    "into",
    "is",
    "it",
    "no",
    "not",
    "of",
    "on",
    "or",
    "such",
    "that",
    "the",
    "their",
    "then",
    "there",
    "these",
    "they",
    "this",
    "to",
    "was",
    "will",
    "with",
    "you",
    "çš„",
    "äº†",
    "åœ¨",
    "æ˜¯",
    "æˆ‘",
    "ä½ ",
    "ä»–",
    "å¥¹",
    "æˆ‘å€‘",
    "ä½ å€‘",
    "ä»–å€‘",
    "ä¹Ÿ",
    "å¾ˆ",
    "éƒ½",
    "ä¸",
    "å°±",
    "æœƒ",
    "å—",
    "å•Š",
    "å§",
    "é€™",
    "é‚£",
    "æœ‰",
    "æ²’æœ‰",
    "ä¸€å€‹",
    "ä»€éº¼",
}


def tokenize_words(text: str) -> List[str]:
    """Return tokens including English words and contiguous Chinese characters."""
    return re.findall(r"[A-Za-z']+|[\u4e00-\u9fff]+", text.lower())


def split_sentences(text: str) -> List[str]:
    parts = [s.strip() for s in re.split(r"[.!?ã€‚ï¼ï¼Ÿ]+", text) if s.strip()]
    return parts if parts else [text.strip()]


def extract_features(text: str) -> Dict[str, float]:
    words = tokenize_words(text)
    word_count = len(words)
    unique_words = len(set(words))
    counts = Counter(words)
    hapax = sum(1 for _, c in counts.items() if c == 1)

    sentences = split_sentences(text)
    sentence_lengths = [len(tokenize_words(s)) for s in sentences]
    sentence_mean = np.mean(sentence_lengths) if sentence_lengths else 0.0
    sentence_std = np.std(sentence_lengths) if sentence_lengths else 0.0
    burstiness = sentence_std / sentence_mean if sentence_mean else 0.0

    chars = [c for c in text if not c.isspace()]
    punctuation = [c for c in chars if unicodedata.category(c).startswith("P")]
    cjk_chars = [c for c in chars if re.match(r"[\u4e00-\u9fff]", c)]
    uppercase = [c for c in chars if c.isupper()]
    digits = [c for c in chars if c.isdigit()]

    stopword_hits = sum(1 for w in words if w in STOP_WORDS)
    long_words = sum(1 for w in words if len(w) >= 7)

    avg_word_length = sum(len(w) for w in words) / word_count if word_count else 0.0
    punctuation_ratio = len(punctuation) / max(len(chars) - len(punctuation), 1)
    punct_per_sentence = len(punctuation) / len(sentences) if sentences else 0.0
    uppercase_ratio = len(uppercase) / len(chars) if chars else 0.0
    digit_ratio = len(digits) / len(chars) if chars else 0.0
    stopword_ratio = stopword_hits / word_count if word_count else 0.0
    lexical_diversity = unique_words / word_count if word_count else 0.0
    long_word_ratio = long_words / word_count if word_count else 0.0
    cjk_ratio = len(cjk_chars) / len(chars) if chars else 0.0
    hapax_ratio = hapax / word_count if word_count else 0.0

    return {
        "avg_word_length": avg_word_length,
        "lexical_diversity": lexical_diversity,
        "stopword_ratio": stopword_ratio,
        "punctuation_ratio": punctuation_ratio,
        "punctuation_per_sentence": punct_per_sentence,
        "uppercase_ratio": uppercase_ratio,
        "digit_ratio": digit_ratio,
        "sentence_mean_len": sentence_mean,
        "sentence_std_len": sentence_std,
        "burstiness": burstiness,
        "long_word_ratio": long_word_ratio,
        "cjk_ratio": cjk_ratio,
        "hapax_ratio": hapax_ratio,
    }


def build_training_samples() -> List[Tuple[str, str]]:
    """Small seed dataset to let the classifier learn a rough boundary."""
    ai_samples = [
        "As an AI language model, I can provide a structured explanation of the topic with clear bullet points and a concise summary.",
        "Below is a step-by-step guide. First, initialize the environment; second, load the data; finally, evaluate the metrics.",
        "I do not possess consciousness or personal opinions. However, I can simulate a reasoning process to help you decide.",
        "The experiment demonstrates that increasing context length improves coherence in downstream tasks such as summarization.",
        "Here are three actionable suggestions: 1) refactor the helper function, 2) add unit tests, and 3) document the interface.",
        "ä½œç‚ºä¸€å€‹æ¨¡å‹ï¼Œæˆ‘å¯ä»¥æä¾›å•é¡Œæ‹†è§£èˆ‡æ­¥é©ŸåŒ–çš„è§£æ³•ï¼Œä¸¦åˆ—å‡ºéœ€è¦æ³¨æ„çš„é‚Šç•Œæ¢ä»¶èˆ‡å‡è¨­ã€‚",
        "ä¸‹æ–¹æ˜¯ç°¡è¦çµè«–ï¼šæ¨¡å‹åœ¨ 15 å€‹ epoch å¾Œæ”¶æ–‚ï¼Œé©—è­‰é›†æŒ‡æ¨™ç©©å®šï¼Œæ¨è«–æ™‚é–“å¯æ§ã€‚",
        "æ­¤æ–¹æ³•é€éæ³¨æ„åŠ›æ©Ÿåˆ¶æ•æ‰é•·è·é—œä¿‚ï¼Œåœ¨æ‘˜è¦ä»»å‹™ä¸Šå„ªæ–¼å‚³çµ±åºåˆ—æ¨¡å‹ã€‚",     
        "é€™ä»½å ±å‘Šæ•´ç†äº†å¯¦é©—çµæœï¼Œé¡¯ç¤ºåœ¨é«˜æº«æ¢ä»¶ä¸‹æ•ˆèƒ½ä»ç¶­æŒç©©å®šï¼Œé©åˆéƒ¨ç½²åœ¨é‚Šç·£è£ç½®ã€‚",
        "ç¸½çµè€Œè¨€ï¼Œç³»çµ±åœ¨ä½è³‡æºç’°å¢ƒä¸‹ä»ç¶­æŒ 92% çš„ F1 åˆ†æ•¸ï¼Œæ¨ç†å»¶é²æ§åˆ¶åœ¨ 80ms ä»¥å…§ã€‚",
        "Data preprocessing includes normalization, deduplication, and linguistic filtering to reduce downstream noise.",
        "ä¸Šç­æ­æ·é‹æ™‚çœ‹åˆ°ä¸€ä½å°æœ‹å‹æŠŠåº§ä½è®“çµ¦è€äººå®¶ï¼Œæ—é‚Šçš„äººéƒ½å¾®ç¬‘é»é ­ï¼Œå¿ƒæƒ…çªç„¶è®Šå¾—å¾ˆå¥½ã€‚",
        "The model outputs show strong convergence after 20 epochs, which suggests the optimizer found a stable minimum.",
    ]
    human_samples = [
        "æ¨¡å‹æ²’æœ‰ä¸»è§€æ„è­˜ï¼Œä½†èƒ½æ ¹æ“šè¼¸å…¥æ¢ä»¶æ¨¡æ“¬æ±ºç­–æµç¨‹ï¼Œæä¾›å¯æ¡å–çš„è¡Œå‹•å»ºè­°ã€‚",
        "ä»¥ä¸‹æµç¨‹å¯åŠ é€Ÿé–‹ç™¼ï¼šå…ˆç”Ÿæˆåˆæˆæ•¸æ“šï¼Œå†ç”¨çœŸå¯¦æ•¸æ“šå¾®èª¿ï¼Œæœ€å¾Œä»¥æ··åˆé›†åšé©—è­‰ã€‚",
        "The model automatically generates a concise brief, highlights the top risks, and recommends three mitigation steps.",
        "I wrote this late at night while drinking coffee, so excuse the rough edges and occasional rambling in the middle.",
        "When I tried the recipe the first time, the dough wouldn't rise, but after leaving it near the stove it finally worked.",
        "Back in college we used to argue for hours about music, then crash on the couch and play whatever old records we had.",
        "The kids ran through the park, laughing as the sprinklers came on and soaked everyone before the sun went down.",
        "I missed the bus, so I walked home in the rain and thought about the conversation we'd had the day before.",
        "It felt like a long day at work, but the moment I opened the window the breeze made everything a little easier.",
        "æ˜¨å¤©æ™šé£¯å¾Œå‡ºé–€æ•£æ­¥ï¼Œè¡—è§’éºµåŒ…åº—é‚„åœ¨çƒ¤éºµåŒ…ï¼Œæ•´æ¢å··å­éƒ½æ˜¯å‰›å‡ºçˆçš„é¦™å‘³ã€‚",    
        "é€±æœ«å’Œæœ‹å‹å»å±±ä¸Šèµ°æ­¥é“ï¼Œçµæœä¸‹äº†å°é›¨ï¼Œæ¨¹è‘‰è¢«æ‰“æ¿•å¾Œåè€Œæ›´ç¶ ï¼Œç©ºæ°£è£¡éƒ½æ˜¯æ³¥åœŸå‘³ã€‚",
        "å‡Œæ™¨è¶•ç¨¿æ™‚è½åˆ°å¤–é¢æœ‰çƒé´‰å«ï¼Œæ‰ç™¼ç¾è‡ªå·±ç†¬å¾—å¤ªæ™šï¼Œæ±ºå®šå…ˆç¡è¦ºæ˜å¤©å†å¯«ã€‚",
        "æ—©ä¸Šæ“ å…¬è»Šæ™‚è¢«é›¨æ»´æ‰“åˆ°ï¼Œå¸æ©Ÿé–‹äº†ä¸€å€‹å¤§è½‰å½ï¼Œå¤§å®¶å·®é»ç«™ä¸ç©©ï¼Œé‚„å¥½æ²’äººå—å‚·ã€‚",
        "ä¸Šé€±æœ«å›è€å®¶ï¼Œå¹«çˆ¸åª½æ•´ç†é™¢å­ï¼Œç¿»åœŸæ™‚ç«Ÿç„¶æŒ–å‡ºä»¥å‰åŸ‹çš„ç»ç’ƒå½ˆç ã€‚",
        "æœ‹å‹è‡¨æ™‚ç´„åƒç«é‹ï¼Œçµæœæ’éšŠæ’äº†å¿«ä¸€å°æ™‚ï¼Œå¤§å®¶é¤“åˆ°å…ˆè²·ç‚¸é›é‚Šç­‰é‚Šåƒã€‚",
        "æ˜¨æ™šç¡å‰çœ‹äº†ä¸€æœ¬èˆŠæ¼«ç•«ï¼Œç¿»åˆ°ä»¥å‰æ‘ºèµ·ä¾†çš„é è§’ï¼Œæ‰æƒ³èµ·é‚£æ™‚å€™å¡åœ¨é‚£æ®µåŠ‡æƒ…ã€‚",
        "He missed his train, wandered around the station, and ended up chatting with a stranger until the next one arrived.",
        "I biked along the river, stopped for a cheap sandwich, and watched the sunset turn the buildings orange.",
        "åœ¨å¤œå¸‚è²·äº†ä¸€æ¯æŸ³æ©™æ±ï¼Œè€æ¿åŠ äº†å¥½å¤šå†°å¡Šï¼Œå–åˆ°æœ€å¾Œé‚„æ˜¯é…¸é…¸ç”œç”œçš„å¾ˆå¥½å–ã€‚",
        "é€™å€‹æ–¹æ³•çš„é™åˆ¶åœ¨æ–¼éœ€è¦å¤§é‡ä¸Šä¸‹æ–‡ï¼Œå¦‚æœæˆªæ–·éçŸ­ï¼Œæ¨¡å‹å¯èƒ½å¿½ç•¥é—œéµç´°ç¯€ã€‚",
        "æ¨¡å‹å°è¼¸å…¥æ ¼å¼æ•æ„Ÿï¼Œå»ºè­°ä¿æŒä¸€è‡´çš„æ®µè½çµæ§‹èˆ‡æ¨™é»ï¼Œä»¥ç²å¾—è¼ƒç©©å®šçš„è¼¸å‡ºã€‚",
    ]

    samples = [("ai", t) for t in ai_samples] + [("human", t) for t in human_samples]
    return samples


@st.cache_resource(show_spinner=False)
def train_model() -> Tuple[any, List[str]]:
    data = build_training_samples()
    feature_rows = []
    labels = []
    for label, text in data:
        feature_rows.append(extract_features(text))
        labels.append(label)

    feature_names = list(feature_rows[0].keys())
    X = pd.DataFrame(feature_rows)
    y = np.array(labels)

    model = make_pipeline(
        StandardScaler(),
        LogisticRegression(max_iter=800, C=2.5, solver="lbfgs", class_weight="balanced"),
    )
    model.fit(X, y)
    return model, feature_names


def predict_text(model, feature_names: List[str], text: str) -> Dict[str, float]:
    features = extract_features(text)
    X = np.array([[features[name] for name in feature_names]])
    proba = model.predict_proba(X)[0]
    label_order = list(model.classes_)
    return {label_order[i]: float(proba[i]) for i in range(len(label_order))}


def render_probability_bar(probabilities: Dict[str, float]) -> None:
    prob_df = (
        pd.DataFrame(
            {
                "Label": list(probabilities.keys()),
                "Probability": [probabilities[k] * 100 for k in probabilities],
            }
        )
        .set_index("Label")
        .sort_values(by="Probability", ascending=False)
    )
    st.bar_chart(prob_df, height=200)


def render_features_table(feature_map: Dict[str, float]) -> None:
    df = pd.DataFrame(
        {
            "Feature": list(feature_map.keys()),
            "Value": [round(v, 4) for v in feature_map.values()],
        }
    )
    st.dataframe(df, use_container_width=True, hide_index=True)


def add_to_history(text: str, probabilities: Dict[str, float]) -> None:
    history = st.session_state.setdefault("history", [])
    predicted_label = max(probabilities, key=probabilities.get)
    history.append(
        {
            "Text (truncated)": (text[:80] + "â€¦") if len(text) > 80 else text,
            "AI %": round(probabilities.get("ai", 0.0) * 100, 2),
            "Human %": round(probabilities.get("human", 0.0) * 100, 2),
            "Winner": predicted_label,
        }
    )
    st.session_state.history = history[-10:]  # keep the last 10 entries


def generate_feature_insights(features: Dict[str, float]) -> List[str]:
    """Return a few heuristic observations to pair with the raw stats."""
    insights: List[str] = []

    if features["burstiness"] < 0.2:
        insights.append("å¥é•·è®Šç•°åº¦ä½ï¼Œå¥å¼è¼ƒå¹³å‡ã€ç¯€å¥å¹³ç©©ã€‚")
    elif features["burstiness"] > 0.6:
        insights.append("å¥é•·è®Šç•°åº¦é«˜ï¼Œå¥å¼é•·çŸ­è½å·®å¤§ã€è¼ƒè‡ªç”±ã€‚")

    if features["lexical_diversity"] < 0.45:
        insights.append("è©å½™å¤šæ¨£æ€§è¼ƒä½ï¼Œå­—è©é‡è¤‡åº¦è¼ƒé«˜ã€‚")
    elif features["lexical_diversity"] > 0.65:
        insights.append("è©å½™å¤šæ¨£æ€§è¼ƒé«˜ï¼Œå­—è©è®ŠåŒ–è±å¯Œã€‚")

    if features["stopword_ratio"] < 0.35:
        insights.append("åŠŸèƒ½è©æ¯”ä¾‹åä½ï¼Œè¡¨é”è¼ƒç²¾ç…‰ã€ç›´è¿°ã€‚")
    elif features["stopword_ratio"] > 0.55:
        insights.append("åŠŸèƒ½è©æ¯”ä¾‹åé«˜ï¼Œèªæ°£è¼ƒå£èªã€é€£æ¥è©è¼ƒå¤šã€‚")

    if features["punctuation_ratio"] < 0.03:
        insights.append("æ¨™é»ä½¿ç”¨å¾ˆå°‘ï¼Œå¥å­å¤šç‚ºç°¡çŸ­ç›´è¿°ã€‚")
    elif features["punctuation_ratio"] > 0.08:
        insights.append("æ¨™é»ä½¿ç”¨è¼ƒå¤šï¼Œèªæ°£æˆ–æƒ…ç·’è½‰æŠ˜è¼ƒé »ç¹ã€‚")

    if features["long_word_ratio"] > 0.35:
        insights.append("é•·è©æ¯”ä¾‹åé«˜ï¼Œå¸¶æœ‰æ­£å¼æˆ–æŠ€è¡“èªªæ˜æ„Ÿã€‚")
    elif features["long_word_ratio"] < 0.22:
        insights.append("é•·è©æ¯”ä¾‹åä½ï¼Œè¼ƒå£èªã€éš¨ç­†çš„ç¯€å¥ã€‚")

    if features["hapax_ratio"] > 0.6:
        insights.append("å¤§é‡åªå‡ºç¾ä¸€æ¬¡çš„è©ï¼Œè¡¨é”è¼ƒéš¨èˆˆã€è®ŠåŒ–å¤šã€‚")
    elif features["hapax_ratio"] < 0.3:
        insights.append("é‡è¤‡è©è¼ƒå¤šï¼Œèªè¨€è¼ƒå…¬å¼åŒ–ã€é‡è¤‡æ€§é«˜ã€‚")

    if not insights:
        insights.append("ç‰¹å¾µåˆ†ä½ˆæ¥è¿‘ä¸­æ€§ï¼Œæ²’æœ‰æ˜é¡¯ AI/Human åå‘ç‰¹å¾µã€‚")

    return insights[:5]


def _set_sample_text(content: str) -> None:
    st.session_state.input_text = content


# Use samples drawn from the actual training set so the model's judgment aligns with buttons.
def pick_sample(kind: str) -> str:
    dataset = build_training_samples()
    pool = [text for label, text in dataset if label == kind]
    return random.choice(pool)


def main() -> None:
    st.title("AI vs Human æ–‡ç« åˆ†é¡å™¨")
    st.caption("è¼¸å…¥ä¸€æ®µæ–‡æœ¬ï¼ŒæŒ‰ä¸‹æŒ‰éˆ•å¾Œä¼°è¨ˆå…¶ AI ç”Ÿæˆæˆ–äººé¡æ’°å¯«çš„æ©Ÿç‡ã€‚")
    st.info("ä½¿ç”¨æµç¨‹ï¼šè¼¸å…¥æˆ–è²¼ä¸Šæ–‡å­— â†’ å¯ç”¨ç¯„ä¾‹å¡«å…… â†’ æŒ‰ã€Œé–‹å§‹åˆ¤æ–·ã€ â†’ æŸ¥çœ‹çµæœèˆ‡ç‰¹å¾µã€‚")

    model, feature_names = train_model()

    col_left, col_right = st.columns([2, 1])

    with col_left:
        st.subheader("è¼¸å…¥æ–‡æœ¬")
        input_text = st.text_area(
            "è²¼ä¸Šæˆ–éµå…¥ä»»æ„æ®µè½",
            value=st.session_state.get("input_text", ""),
            height=180,
            key="input_text",
        )

        sample_col1, sample_col2 = st.columns(2)
        with sample_col1:
            st.button("å¡«å…¥ AI ç¯„ä¾‹", on_click=lambda: _set_sample_text(pick_sample("ai")))
        with sample_col2:
            st.button("å¡«å…¥ Human ç¯„ä¾‹", on_click=lambda: _set_sample_text(pick_sample("human")))
        st.caption("ç¯„ä¾‹éš¨æ©Ÿå–è‡ªæ¨¡å‹çš„è¨“ç·´æ¨£æœ¬ï¼Œæ–¹ä¾¿å¿«é€Ÿè©¦è·‘ä¸¦èˆ‡æ¨¡å‹è¡Œç‚ºå°é½Šã€‚")

        analyze = st.button("é–‹å§‹åˆ¤æ–·")

        st.markdown("---")

        result = st.session_state.get("last_result")
        if analyze and input_text.strip():
            probabilities = predict_text(model, feature_names, input_text)
            features = extract_features(input_text)
            add_to_history(input_text, probabilities)
            st.session_state["last_result"] = {
                "probabilities": probabilities,
                "features": features,
                "text": input_text,
            }
            result = st.session_state["last_result"]
        elif analyze and not input_text.strip():
            st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—å†æŒ‰ä¸‹é–‹å§‹åˆ¤æ–·ã€‚")

        if result:
            probs = result["probabilities"]
            st.subheader("åˆ¤æ–·çµæœ")
            ai_pct = probs.get("ai", 0.0) * 100
            human_pct = probs.get("human", 0.0) * 100
            top_label = max(probs, key=probs.get) if probs else "-"
            col_a, col_b = st.columns(2)
            col_a.metric("AI æ©Ÿç‡", f"{ai_pct:.1f}%")
            col_b.metric("Human æ©Ÿç‡", f"{human_pct:.1f}%")
            render_probability_bar(probs)
            st.caption(f"æœ€é«˜æ©Ÿç‡ï¼š{top_label.upper()}ï¼ˆåƒ…ä¾›åƒè€ƒï¼Œè«‹æ­é…äººå·¥åˆ¤æ–·ï¼‰")
        else:
            st.info("è¼¸å…¥æ–‡å­—å¾ŒæŒ‰ä¸‹ã€Œé–‹å§‹åˆ¤æ–·ã€å³å¯çœ‹åˆ°çµæœã€‚")

    with col_right:
        st.subheader("è¼¸å…¥çµ±è¨ˆ / ç‰¹å¾µ")
        result = st.session_state.get("last_result")
        if result:
            feature_map = result["features"]
            with st.expander("ç‰¹å¾µè¡¨èˆ‡è§£è®€", expanded=True):
                render_features_table(feature_map)
                insights = generate_feature_insights(feature_map)
                st.markdown("**ç‰¹å¾µè§€å¯Ÿ**")
                for note in insights:
                    st.markdown(f"- {note}")
        else:
            st.write("ç­‰å¾…è¼¸å…¥ä¸­â€¦")

        st.markdown("---")
        hist_title_col, hist_btn_col = st.columns([3, 1])
        with hist_title_col:
            st.subheader("æœ€è¿‘æ¨è«–")
        with hist_btn_col:
            if st.button("æ¸…ç©ºç´€éŒ„"):
                st.session_state["history"] = []
        history = st.session_state.get("history", [])
        if history:
            st.dataframe(pd.DataFrame(history), use_container_width=True, hide_index=True, height=240)
        else:
            st.caption("å°šç„¡ç´€éŒ„ï¼Œè¼¸å…¥æ–‡å­—å¾Œæœƒè‡ªå‹•é¡¯ç¤ºé€™è£¡ã€‚")

        st.markdown("---")
        st.subheader("æ¨¡å‹èªªæ˜")
        st.write(
            "æ­¤åˆ†é¡å™¨ä½¿ç”¨å°å‹æ¨£æœ¬ï¼ŒåŸºæ–¼æ–‡å­—ç‰¹å¾µï¼ˆè©é•·ã€å¤šæ¨£æ€§ã€æ¨™é»æ¯”ä¾‹ã€å¥é•·è®Šç•°ã€CJK æ¯”ä¾‹ã€hapax ç­‰ï¼‰"
            "è¨“ç·´é‚è¼¯è¿´æ­¸æ¨¡å‹ã€‚çµæœåƒ…ä¾›åƒè€ƒï¼Œå»ºè­°æ­é…äººå·¥å¯©æ ¸ã€‚"
        )


if __name__ == "__main__":
    main()
